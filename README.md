

# 🧠 Keyword Identification Project

Automated **keyword extraction** and **visualization** from institutional websites using *Python NLP*, *web scraping*, and *data analysis*.  
This project analyzes and compares the main topics and keyword structures for sites like **IIM Ahmedabad** and **IISER Mohali**.

***

## ✨ Features

- Web scraping, text cleaning, and lemmatization  
- Keyword frequency and TF-IDF analysis per site  
- Visualizations: bar charts, word clouds, heatmaps, and co-occurrence networks  
- SQLite database for efficient storage and text reuse  

***

## 🖼 Example Outputs

These are the types of results generated by the project:

### Top Words Bar Charts
![IIM Ahmedabad Bar Chart](IIM_Ahmedabad_bar_chart.jpeg)
![IISER Mohali Bar Chart](IISER_Mohali_bar_chart.jpeg)

### Word Clouds
![IIM Ahmedabad Word Cloud](IIM_Ahmedabad_wordcloud.jpeg)
![IISER Mohali Word Cloud](IISER_Mohali_wordcloud.jpeg)

### TF-IDF Heatmap
![TF-IDF Heatmap](tfidf_heatmap.jpeg)

### Word Co-occurrence Networks
![IIM Ahmedabad Network](IIM_Ahmedabad_network.jpeg)
![IISER Mohali Network](IISER_Mohali_network.jpeg)


***

## ⚙️ Getting Started

### 1. Prerequisites

- Python 3.x  
- Recommended: virtual environment (`venv`)

### 2. Installation

Clone the repository and install dependencies:

```bash
git clone https://github.com/<your-username>/Key-Word-Identification-.git
cd Key-Word-Identification-
pip install -r requirements.txt
```

### 3. requirements.txt

```txt
newspaper3k
requests
beautifulsoup4
tqdm
scikit-learn
lxml
seaborn
nltk
pandas
matplotlib
networkx
wordcloud
```

***

## 🚀 Usage

Run the main analysis script:

```bash
python main.py
```

The script will:
- Crawl and analyze a set number of pages (`MAX_PAGES`)  
- Create or update the database `keyword_project_enhanced.db`  
- Generate all output images in the working directory  

To automate commits and pushes from Windows, run:
```bash
submit.bat
```

***

## 📁 Project Files

| Filename | Description |
|-----------|-------------|
| `main.py` | Main analysis script for scraping, keyword detection, and visualization |
| `requirements.txt` | Python libraries required for execution |
| `submit.bat` | Batch file to automate Git commits and pushes |
| `keyword_project_enhanced.db` | SQLite database storing all scraped and cleaned webpage text |
| *(Output images)* | Generated visualizations (bar charts, word clouds, heatmaps, etc.) |

***

## 🧩 keyword_project_enhanced.db

- Automatically created and updated by `main.py`  
- Stores scraped, cleaned, and processed web text from both institutions  
- Prevents redundant scraping between runs  
- Delete the `.db` file to start fresh  
- For large files, consider adding it to `.gitignore` instead of uploading to GitHub  

***

## 📜 Code Overview

- **`main.py`** – Complete workflow: crawl → preprocess → analyze → visualize  
- **`requirements.txt`** – Python dependencies  
- **`submit.bat`** – Git automation for Windows users  

***

## 🔍 How It Works

1. **Crawling** – Scrapes internal web pages of each institution  
2. **Text Processing** – Cleans text, removes stopwords, and lemmatizes words  
3. **Database Storage** – Saves processed text to `keyword_project_enhanced.db`  
4. **Analysis** – Performs keyword frequency and TF-IDF calculations  
5. **Visualization** – Generates bar charts, word clouds, heatmaps, and networks  

***

## 🛠 Customization

- Adjust `MAX_PAGES` in `main.py` to control scraping depth  
- Add more websites to the `SITES` dictionary in `main.py`  
- Modify visualization styles and color maps in plotting functions  

***

## 🙌 Acknowledgments

Built using the following open-source libraries:  
**BeautifulSoup**, **requests**, **nltk**, **pandas**, **matplotlib**, **seaborn**, **wordcloud**, **networkx**, **scikit-learn**, and **newspaper3k**.

***
